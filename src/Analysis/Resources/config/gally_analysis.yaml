gally:
    analysis:
        char_filters:
            html_strip:
                type: html_strip
        filters:
            trim:
                type: trim
            # Absolute max supported by Lucene for a token length is 32766 bytes, 
            # so using 1/4 of that to accommodate multibyte UTF-8 characters
            truncate_to_max:
                type: truncate
                params:
                    length: 8192
            lowercase:
                type: lowercase
            word_delimiter:
                type: word_delimiter
                params:
                    generate_word_parts: true
                    catenate_words: true
                    catenate_numbers: true
                    catenate_all: true
                    split_on_case_change: true
                    split_on_numerics: true
                    preserve_original: true
            shingle:
                type: shingle
                params:
                    min_shingle_size: 2
                    max_shingle_size: 2
                    output_unigrams: true
            reference_shingle:
                type: shingle
                params:
                    min_shingle_size: 2
                    max_shingle_size: 10
                    output_unigrams: true
                    token_separator: ""
            reference_word_delimiter:
                type: word_delimiter
                params:
                    generate_word_parts: true
                    catenate_words: false
                    catenate_numbers: false
                    catenate_all: false
                    split_on_case_change: true
                    split_on_numerics: true
                    preserve_original: false
            ascii_folding:
                type: asciifolding
                params:
                    preserve_original: false
            phonetic:
                type: phonetic
                params:
                    encoder: metaphone
            edge_ngram_filter:
                type: edge_ngram
                params:
                    min_gram: 3
                    max_gram: 20
        analyzers:
            keyword:
                char_filter: ~
                tokenizer: keyword
                filter: 
                    - truncate_to_max
            standard:
                char_filter:
                    - html_strip
                tokenizer: standard
                filter:
                    - ascii_folding
                    - trim
                    - elision
                    - word_delimiter
                    - lowercase
                    - stemmer_override
                    - stemmer
            whitespace:
                char_filter:
                    - html_strip
                tokenizer: standard
                filter:
                    - ascii_folding
                    - trim
                    - elision
                    - word_delimiter
                    - lowercase
            reference:
                char_filter:
                    - html_strip
                tokenizer: standard
                filter:
                    - ascii_folding
                    - trim
                    - elision
                    - reference_word_delimiter
                    - lowercase
                    - reference_shingle
            shingle:
                char_filter:
                    - html_strip
                tokenizer: whitespace
                filter:
                    - ascii_folding
                    - trim
                    - elision
                    - word_delimiter
                    - lowercase
                    - stemmer_override
                    - stemmer
                    - shingle
            sortable:
                char_filter:
                    - html_strip
                tokenizer: keyword
                filter:
                    - ascii_folding
                    - trim
                    - lowercase
            phonetic:
                char_filter:
                    - html_strip
                tokenizer: standard
                filter:
                    - ascii_folding
                    - trim
                    - elision
                    - word_delimiter
                    - lowercase
                    - phonetic
            standard_edge_ngram:
                char_filter:
                    - html_strip
                tokenizer: standard
                filter:
                    - ascii_folding
                    - trim
                    - word_delimiter
                    - lowercase
                    - elision
                    - stemmer_override
                    - stemmer
                    - edge_ngram_filter
